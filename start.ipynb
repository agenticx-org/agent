{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from search.helpers import *\n",
    "from bertopic import BERTopic\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22767/22767 [00:00<00:00, 118963.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = load_dataset(\"smartcat/Amazon_Products_2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'images', 'videos', 'store', 'categories', 'parent_asin', 'item_weight', 'brand', 'item_model_number', 'product_dimensions', 'batteries_required', 'color', 'material', 'material_type', 'style', 'number_of_items', 'manufacturer', 'package_dimensions', 'date_first_available', 'best_sellers_rank', 'age_range_(description)'],\n",
       "    num_rows: 22767\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = dataset['train']['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_model.encode(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 11:48:27,748 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-09 11:48:45,257 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-04-09 11:48:45,259 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-09 11:48:48,618 - BERTopic - Cluster - Completed âœ“\n",
      "2025-04-09 11:48:48,626 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-04-09 11:48:49,521 - BERTopic - Representation - Completed âœ“\n",
      "2025-04-09 11:48:49,925 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topic  Count                             Name  \\\n",
      "0      -1   6555                -1_the_and_to_for   \n",
      "1       0   2842           0_wall_the_to_stickers   \n",
      "2       1   1737      1_blanket_baby_and_blankets   \n",
      "3       2   1573           2_cup_silicone_and_the   \n",
      "4       3   1469             3_bag_diaper_and_the   \n",
      "5       4   1061          4_skin_baby_and_formula   \n",
      "6       5    978       5_diapers_diaper_cloth_and   \n",
      "7       6    902        6_storage_basket_toys_and   \n",
      "8       7    799     7_crib_sheet_sheets_mattress   \n",
      "9       8    758   8_hair_headbands_bows_headband   \n",
      "10      9    733          9_locks_lock_the_safety   \n",
      "11     10    517     10_size_color_length_package   \n",
      "12     11    391  11_monitor_camera_power_battery   \n",
      "13     12    306             12_bibs_bib_baby_and   \n",
      "14     13    302      13_towel_bath_hooded_towels   \n",
      "15     14    299          14_light_night_lamp_usb   \n",
      "16     15    288       15_stool_toilet_potty_step   \n",
      "17     16    243                           16____   \n",
      "18     17    228                17_bib_my_me_baby   \n",
      "19     18    210      18_pump_breast_milk_pumping   \n",
      "20     19    208        19_pillow_nursing_and_for   \n",
      "21     20    198      20_nail_nails_baby_grinding   \n",
      "22     21    170       21_cards_shower_thank_card   \n",
      "\n",
      "                                       Representation  \\\n",
      "0   [the, and, to, for, of, is, your, with, baby, in]   \n",
      "1   [wall, the, to, stickers, surface, and, you, r...   \n",
      "2   [blanket, baby, and, blankets, for, the, your,...   \n",
      "3   [cup, silicone, and, the, for, to, food, with,...   \n",
      "4   [bag, diaper, and, the, seat, car, to, your, y...   \n",
      "5   [skin, baby, and, formula, with, is, to, wipes...   \n",
      "6   [diapers, diaper, cloth, and, to, for, the, ar...   \n",
      "7   [storage, basket, toys, and, laundry, toy, for...   \n",
      "8   [crib, sheet, sheets, mattress, fitted, and, t...   \n",
      "9   [hair, headbands, bows, headband, bow, for, ba...   \n",
      "10  [locks, lock, the, safety, to, furniture, cabi...   \n",
      "11  [size, color, length, package, material, due, ...   \n",
      "12  [monitor, camera, power, battery, baby, the, a...   \n",
      "13  [bibs, bib, baby, and, to, the, for, with, you...   \n",
      "14  [towel, bath, hooded, towels, baby, washcloths...   \n",
      "15  [light, night, lamp, usb, led, the, power, mod...   \n",
      "16  [stool, toilet, potty, step, seat, training, t...   \n",
      "17                               [, , , , , , , , , ]   \n",
      "18  [bib, my, me, baby, cute, grandma, gift, loves...   \n",
      "19  [pump, breast, milk, pumping, nipple, breastfe...   \n",
      "20  [pillow, nursing, and, for, support, toddler, ...   \n",
      "21  [nail, nails, baby, grinding, brush, file, spe...   \n",
      "22  [cards, shower, thank, card, you, baby, invita...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [TRANSFORM YOUR CHILDâ€™S BEDROOM, NURSERY OR AN...  \n",
      "1   [Features:\\nThere are four monkeys playing on ...  \n",
      "2   [When it comes to caring for your newborn, bab...  \n",
      "3   [10pcs Baby Feeding Set | Baby Led Weaning Ute...  \n",
      "4   [âœ“ MODERN STYLE BABY DIAPER BAG BACKPACK PINK ...  \n",
      "5   [Use JOHNSON'S HEAD-TO-TOE Baby Lotion as part...  \n",
      "6   [Feature\\nStyle of diaper: One size Pocket Dig...  \n",
      "7   [Felt storage bins is popular way to add some ...  \n",
      "8   [Are you tired of those crib sheets that keep ...  \n",
      "9   [Features:\\n1.Elegant unique designed Fantasti...  \n",
      "10  [Are you worried about your kidsâ€™ curiosity ge...  \n",
      "11  [Specification: Condition: 100% Brand New Mate...  \n",
      "12  [AXVUE E600 Video Baby Monitor, Slim Handheld,...  \n",
      "13  [Norinori baby Bandana Bibs are made with 100%...  \n",
      "14  [BEST ORGANIC MUSLIN BATH TOWEL FOR YOUR BABY\\...  \n",
      "15  [What is 3D optical night light? ---The name i...  \n",
      "16  [KOADOA Dual Height 2 Step Stool for children,...  \n",
      "17                                          [1, 1, 1]  \n",
      "18  [Embroidered first birthday personalized bib f...  \n",
      "19  [ðŸ˜ŠWhy choose a momauro breast pump?ðŸ˜Š Studies h...  \n",
      "20  [Clouds Collection nursing pillow slipcovers\\n...  \n",
      "21  [Nail Clippers Nail File Nail Trimmer Electric...  \n",
      "22  [With this pack of Girls Woodland Baby Shower ...  \n"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic(\n",
    "\n",
    "# Pipeline models\n",
    "embedding_model=embedding_model,\n",
    "umap_model=umap_model,\n",
    "hdbscan_model=hdbscan_model,\n",
    "#vectorizer_model=vectorizer_model,\n",
    "#representation_model=representation_model,\n",
    "calculate_probabilities=False,\n",
    "# Hyperparameters\n",
    "top_n_words=10,\n",
    "verbose=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "# Show topics\n",
    "print(topic_model.get_topic_info())\n",
    "\n",
    "topic_model.save(\"bert_topic.pkl\", serialization=\"pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 96.10it/s]\n"
     ]
    }
   ],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parent_ID</th>\n",
       "      <th>Parent_Name</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Child_Left_ID</th>\n",
       "      <th>Child_Left_Name</th>\n",
       "      <th>Child_Right_ID</th>\n",
       "      <th>Child_Right_Name</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>the_to_of_and_ax</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>the_to_of_and_ax</td>\n",
       "      <td>1</td>\n",
       "      <td>____</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Parent_ID       Parent_Name  Topics Child_Left_ID   Child_Left_Name  \\\n",
       "0         2  the_to_of_and_ax  [0, 1]             0  the_to_of_and_ax   \n",
       "\n",
       "  Child_Right_ID Child_Right_Name  Distance  \n",
       "0              1             ____       1.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchical_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic.load(\"/Users/edenduthie/Documents/code/agent/bert_topic.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 11:26:30,598 - ERROR - An unexpected error occurred during execution: 401 Client Error. (Request ID: Root=1-67f5ccc6-4b9467d84d43d81c6f148259;39d87f27-d6d2-478e-80a8-9b36ccaf3b78)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/old/bert_topic.pkl/resolve/main/topics.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Invalid username or password.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/old/bert_topic.pkl/resolve/main/topics.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/pn/rbnq4l_j5v31cppkdrccyx_80000gn/T/ipykernel_97862/4165022168.py\", line 679, in <module>\n",
      "    signal_search_instance = SignalSearch(load_local_model=True, model_path=\"old/bert_topic.pkl\")\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/pn/rbnq4l_j5v31cppkdrccyx_80000gn/T/ipykernel_97862/4165022168.py\", line 35, in __init__\n",
      "    self.topic_model = BERTopic.load(model_path)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/bertopic/_bertopic.py\", line 3428, in load\n",
      "    topics, params, tensors, ctfidf_tensors, ctfidf_config, images = save_utils.load_files_from_hf(path)\n",
      "                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/bertopic/_save_utils.py\", line 229, in load_files_from_hf\n",
      "    topics = load_cfg_from_json(hf_hub_download(path, TOPICS_NAME, revision=None))\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 961, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1068, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1596, in _raise_on_head_call_error\n",
      "    raise head_call_error\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1484, in _get_metadata_or_catch_error\n",
      "    metadata = get_hf_file_metadata(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1401, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 285, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 309, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/edenduthie/Documents/code/agent/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 459, in hf_raise_for_status\n",
      "    raise _format(RepositoryNotFoundError, message, response) from e\n",
      "huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-67f5ccc6-4b9467d84d43d81c6f148259;39d87f27-d6d2-478e-80a8-9b36ccaf3b78)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/old/bert_topic.pkl/resolve/main/topics.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Invalid username or password.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 11:31:09,676 - INFO - Downloading 20news dataset. This may take a few minutes.\n",
      "2025-04-09 11:31:09,676 - INFO - Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
      "2025-04-09 11:31:35,288 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "2025-04-09 11:31:35,293 - INFO - Use pytorch device_name: mps\n",
      "2025-04-09 11:31:35,293 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Batches:   7%|â–‹         | 39/589 [00:18<04:27,  2.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m docs = fetch_20newsgroups(subset=\u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m,  remove=(\u001b[33m'\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfooters\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mquotes\u001b[39m\u001b[33m'\u001b[39m))[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      5\u001b[39m topic_model = BERTopic(verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m topics, probs = \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/agent/.venv/lib/python3.12/site-packages/bertopic/_bertopic.py:454\u001b[39m, in \u001b[36mBERTopic.fit_transform\u001b[39m\u001b[34m(self, documents, embeddings, images, y)\u001b[39m\n\u001b[32m    452\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mEmbedding - Transforming documents to embeddings.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    453\u001b[39m     \u001b[38;5;28mself\u001b[39m.embedding_model = select_backend(\u001b[38;5;28mself\u001b[39m.embedding_model, language=\u001b[38;5;28mself\u001b[39m.language, verbose=\u001b[38;5;28mself\u001b[39m.verbose)\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDocument\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocument\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mEmbedding - Completed \u001b[39m\u001b[38;5;130;01m\\u2713\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/agent/.venv/lib/python3.12/site-packages/bertopic/_bertopic.py:3711\u001b[39m, in \u001b[36mBERTopic._extract_embeddings\u001b[39m\u001b[34m(self, documents, images, method, verbose)\u001b[39m\n\u001b[32m   3709\u001b[39m     embeddings = \u001b[38;5;28mself\u001b[39m.embedding_model.embed_words(words=documents, verbose=verbose)\n\u001b[32m   3710\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3711\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m documents[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3714\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMake sure to use an embedding model that can either embed documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3715\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor images depending on which you want to embed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3716\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/agent/.venv/lib/python3.12/site-packages/bertopic/backend/_base.py:62\u001b[39m, in \u001b[36mBaseEmbedder.embed_documents\u001b[39m\u001b[34m(self, document, verbose)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, document: List[\u001b[38;5;28mstr\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> np.ndarray:\n\u001b[32m     51\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Embed a list of n words into an n-dimensional\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m    matrix of embeddings.\u001b[39;00m\n\u001b[32m     53\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m \u001b[33;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/agent/.venv/lib/python3.12/site-packages/bertopic/backend/_sentencetransformers.py:84\u001b[39m, in \u001b[36mSentenceTransformerBackend.embed\u001b[39m\u001b[34m(self, documents, verbose)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, documents: List[\u001b[38;5;28mstr\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> np.ndarray:\n\u001b[32m     73\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03m    matrix of embeddings.\u001b[39;00m\n\u001b[32m     75\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/agent/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:714\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    712\u001b[39m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[32m    713\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[32m--> \u001b[39m\u001b[32m714\u001b[39m                 embeddings = \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    716\u001b[39m         all_embeddings.extend(embeddings)\n\u001b[32m    718\u001b[39m all_embeddings = [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np.argsort(length_sorted_idx)]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))[\"data\"]\n",
    "topic_model = BERTopic(verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
